# Отчет по семинару № 3
Исследование поведения серверов flask и gunicorn под разными видами нагруки.  

## Введение
Для тяжелых моделей предиктивной аналитики возможно два варианта деплоя. 
Первый вариант - запускать модели на своем сервере. 
Этот вариант имеет очевидный недостаток. 
Если у вас очень тяжелая модель, то пользователи вашего сервиса должны будут долго ждать ответа.  
Даже самый мощный компьютер имеет предел вычислительной мощности. 
Поэтому если вашим сервисом будут пользоваться несколько пользователей одновременно, придется настраивать собственный вычислительный кластер. 

Второй вариант - использовать специальные сервисы, например:  
- TensorFlow Serving
- AWS SageMaker
- Yandex DataSphere
- Google Vertex AI

В этом случае вычислительная нагрузка снимается с вашего сервера. 
Но за каждый запрос к стороннему сервису нужно платить, как деньги, так и временем на обработку запросов. 

## Метод исследования
В файле `src/utils.py` определены три функции, которые эмулируют три варианта решения задачи `predict` :
- `predict_io_bounded(area)` - соответсвует второму варианту, запрос к стороннему сервису заменяет `time.sleep(1)`. 
Это соответствует задержке в 1 секунду, которая нужна для обмена информацией со сторонним сервисом. 
При этом вычислительная нагрузка на наш сервер не создается, процесс просто спит. 
- `predict_cpu_bounded(area, n)` - соответствует первому варианту, предикту на собственном сервере. 
Параметр `n` позволяет регулировать нагрузку, на самом деле это просто вычисление среднего арифметического линейного массива. 
При достаточно больших `n` сервер будет выдавать ошибку из-за нехватки памяти. 
Необходимо эмпирическим путем определить это значение. 
- `predict_cpu_multithread(area, n)` - тоже соответствует первому варианту, но используется оптимизированный код на numpy. 
Необходимо также эмпирическим путем определить критическое значение `n` и сравнить его с предыдущим. 

Для запуска сервиса доступно два варианта: 
- `python src/predict_app.py` - сервер, предназначенный для разработки. 
- `gunicorn src.predict_app:app` - сервер, предназначенный для непрерывной работы в продакшн. 

Нагрузка создается файлом `test/test_parallel.py`.  

**Задача**: запустить 6 (шесть) возможных вариантов сочетаний серверов и функций под нагрузкой в 10 запросов. 

Результат запуска должен быть сохранен в логи, например с помощью перенаправления вывода:  
`python test/test_parallel.py > log/test_np_flask.txt` 
Обратите внимание, файлы должны иметь расширение txt, а значит не игнорятся гитом и должны быть запушены в мастере.  

## Результат и обсуждение
## flask - вычисления выполняются последовательно

### [predict_cpu_bounded](../log/test_np_flask_1.txt)
- Комментарий: В сумме 10 запросов выполняются за 0.5 сек.
- Критическое значение n: 300000000

### [predict_cpu_multithread](../log/test_np_flask_2.txt)
- Комментарий: В сумме 10 запросов выполняются за 32.6 сек.
- Критическое значение n: 300000000

### [predict_io_bounded](../log/test_np_flask_3.txt)
- Комментарий: В сумме 10 запросов выполняются за 1.1 сек.

## unicorn - вычисления выполняются параллельно

### [predict_cpu_bounded](../log/test_np_unicorn_1.txt)
- Комментарий: В сумме 10 запросов выполняются за 16.7 сек.
- Критическое значение n: 300000000

### [predict_cpu_multithread](../log/test_np_unicorn_2.txt)
- Комментарий: В сумме 10 запросов выполняются за 18 сек.
- Критическое значение n: 300000000

### [predict_io_bounded](../log/test_np_unicorn_3.txt)
- Комментарий: В сумме 10 запросов выполняются за 17 сек.

## Выводы

На основании проведенного исследования и результатов логов можно сделать следующие выводы:

1. **Flask сервер**:
   - **`predict_cpu_bounded`**: Обработка 10 запросов выполняется последовательно за 0.5 сек при критическом значении `n = 300000000`. Сервер Flask быстро справляется с этой задачей, но это связано с тем, что нагрузка на CPU для одного запроса относительно низкая.
   - **`predict_cpu_multithread`**: Обработка 10 запросов занимает 32.6 сек при критическом значении `n = 300000000`. Последовательная обработка на Flask приводит к значительным задержкам при высокой нагрузке на CPU.
   - **`predict_io_bounded`**: Обработка 10 запросов занимает 1.1 сек, что подтверждает, что Flask сервер эффективно справляется с задачами ввода-вывода, но ограничен в возможностях параллельной обработки запросов.

2. **Gunicorn сервер**:
   - **`predict_cpu_bounded`**: Обработка 10 запросов выполняется за 16.7 сек при критическом значении `n = 300000000`. Параллельная обработка на Gunicorn позволяет значительно снизить время выполнения запросов по сравнению с Flask.
   - **`predict_cpu_multithread`**: Обработка 10 запросов занимает 18 сек при критическом значении `n = 300000000`. Оптимизированный многопоточный код на Gunicorn показывает лучшее время по сравнению с Flask, но все еще медленнее, чем `predict_cpu_bounded`.
   - **`predict_io_bounded`**: Обработка 10 запросов занимает 17 сек, что значительно медленнее по сравнению с Flask, но позволяет параллельно обрабатывать несколько запросов, что делает его более устойчивым к нагрузкам ввода-вывода.

### Различия между Flask и Gunicorn:
- **Flask**: Лучше подходит для разработки и тестирования. Эффективен для задач с низкой нагрузкой на CPU и ввод-вывод. Ограничен в возможностях параллельной обработки запросов, что приводит к значительным задержкам при высокой нагрузке.
- **Gunicorn**: Предназначен для использования в продакшн среде. Эффективно справляется с параллельной обработкой запросов, что значительно улучшает производительность при высокой нагрузке на CPU и ввод-вывод. Однако требует дополнительных настроек для оптимизации производительности.

Для задач, связанных с высокой вычислительной нагрузкой на CPU или частыми операциями ввода-вывода, рекомендуется использовать Gunicorn сервер в продакшн среде. Flask сервер лучше использовать для разработки и тестирования.
