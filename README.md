# Предиктивная аналитика больших данных

Учебный проект для демонстрации основных этапов жизненного цикла проекта предиктивной аналитики.  

## Installation 

Клонируйте репозиторий, создайте виртуальное окружение, активируйте и установите зависимости:  

```sh
git clone https://github.com/apossya/pabd24
cd pabd24
python -m venv venv

source venv/bin/activate  # mac or linux
.\venv\Scripts\activate   # windows

pip install -r requirements.txt
```

## Usage

### 1. Сбор данных о ценах на недвижимость 
Отработал скрипт parse_cian, в котором создается объект moscow_parser класса CianParser для парсинга данных по недвижимости в Москве.
Затем пределяется функция main, которая выполняет следующие действия:
1. Получает текущее время и форматирует его в строку вида "ГГГГ-ММ-ДД_ЧЧ-ММ".
2. Устанавливает количество комнат (n_rooms) для поиска (3 комнаты).
3. Определяет путь для сохранения CSV файла, включающий количество комнат и текущее время.
4. Использует объект moscow_parser для получения данных о квартирах, указав тип сделки ("продажа"), количество комнат, и дополнительные настройки (начальная и конечная страницы, тип объекта).
5. Преобразует полученные данные в DataFrame с помощью pandas.
6. Сохраняет DataFrame в CSV файл по указанному пути с кодировкой 'utf-8' без индексов.

### 2. Выгрузка данных в хранилище S3 
Для доступа к хранилищу скопируйте файл `.env` в корень проекта.  

Отработал скрипт upload_to_s3, в котором после определения путей и загрузки конфигурации из файла .env определяется функция main, которая выполняет действия:
1. Создает клиент для взаимодействия с S3-хранилищем с помощью boto3.client, используя URL-адрес для Yandex Cloud и ключи доступа из конфигурации.
2. Перебирает пути к файлам, указанные в аргументах командной строки (или использует значение по умолчанию), и для каждого файла формирует имя объекта в S3, добавляя идентификатор пользователя и заменяя обратные слеши на прямые.
3. Загружает файл в S3-хранилище с помощью метода upload_file клиента boto3.
Затем создается парсер аргументов командной строки с помощью argparse.ArgumentParser, который передаёт аргументы командной строки в функцию main.

### 3. Загрузка данных из S3 на локальную машину  
Отработал скрипт download_from_s3, в котором создается клиент boto3 для взаимодействия с S3-хранилищем и определяется функция main, которая выполняет действия:
1. Перебирает пути к файлам, указанные в аргументах командной строки (или использует значение по умолчанию), и для каждого файла формирует имя объекта в S3, добавляя идентификатор пользователя и заменяя обратные слеши на прямые.
2. Загружает файл из S3-хранилища на локальный компьютер с помощью метода download_file клиента boto3.
Затем снова создается парсер аргументов командной строки с помощью argparse.ArgumentParser, который передаёт аргументы командной строки в функцию main.

### 4. Предварительная обработка данных  

Скрипт preprocess_data:
1. Логирование настроено для записи сообщений в файл ./log/preprocess_data.log с использованием кодировки utf-8 и уровня логирования DEBUG.
2. Определены входные и выходные файлы, считаны данные.
3. Добавлен новый столбец url_id, извлечённый из URL.
4. Создан новый DataFrame new_dataframe с колонками url_id, total_meters и price, и установлен индекс по url_id.
5. Удалены строки, где цена превышает PRICE_THRESHOLD.
6. Данные разделены на тренировочный и тестовый наборы:
7. Вычислена граница border для разделения данных согласно заданной доле args.split.
8. Данные разделяются на train_df (тренировочный набор) и val_df (валидационный набор).
9. В зависимости от значения args.split, данные сохранены в соответствующие файлы (train.csv и test.csv):
10. Логирована информация о выполнении записи входных файлов в тренировочный и тестовый датасеты.
11. Создан парсер аргументов командной строки, добавлены параметры --split и --input, и вызвана функция main с аргументами, переданными из командной строки.

### 5. Обучение модели 

Скрипт train_model:
1. Логирование настроено для записи сообщений в файл ./log/train_model.log с использованием кодировки utf-8 и уровня логирования DEBUG.
2. Определены константы, загружены и подготовлены данные:
  * x_train — входные данные для модели, в данном случае только колонка total_meters.
  * y_train — целевые значения (цены), которые модель должна предсказывать.
3. Создан экземпляр модели RandomForestRegressor с 100 деревьями и фиксированным значением random_state для воспроизводимости результатов. Модель обучена на данных x_train и y_train и сохранена.
4. Вычислены метрики качества модели:
  * r2 — коэффициент детерминации, показывающий, насколько хорошо модель объясняет вариацию в данных.
  * mae — средняя абсолютная ошибка, показывающая среднюю величину ошибки предсказаний модели.
5. Создан парсер аргументов командной строки, добавлен параметр --model, и вызвана функция main с аргументами, переданными из командной строки.

### 6. Запуск приложения flask gunicorn
Приложение для предсказания запущено на серверах:
1. flask: http://176.109.104.141:5000/
2. gunicorn: http://176.109.104.141:8000/
3. Токен для авторизации: user1

### 7. Использование сервиса через веб интерфейс 

Для использования сервиса используйте файл `web/index.html`. 
